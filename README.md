<div align="center">
  <h2>AutoJudger: An Agent-Driven Framework for Efficient Benchmarking of MLLMs</h2>
</div>

## 📝 Introduction

<p align="center">
  <img src="resources/Framework.png" width="100%">
</p>

**AutoJudger** is an agent-driven framework for efficient and adaptive benchmarking of MLLMs that tackles this escalating cost. AutoJudger employs the Item Response Theory (IRT) to estimate the question difficulty and an autonomous evaluation agent to dynamically select the most informative test questions based on the model’s real-time performance. Specifically, AutoJudger incorporates two pivotal components: a semantic-aware retrieval mechanism to ensure that selected questions cover diverse and challenging scenarios across both vision and language modalities, and a dynamic memory that maintains contextual statistics of previously evaluated questions to guide coherent and globally informed question selection throughout the evaluation process.Extensive experiments on four representative multimodal benchmarks demonstrate that our adaptive framework dramatically reduces evaluation expenses, i.e. AutoJudger uses only 4% of the data to achieve over 90% ranking accuracy with the full benchmark evaluation on MMT-Bench. 

---

## 🛠️ Getting Started

### Directory Structure
```
AutoJudger/
├── models/                  # Model weights
├── data/                    # Processed benchmark info (splits, difficulty scores)
│   └── SEEDBench_IMG/       # Example: SEEDBench-specific data
├── LMUData/                 # Raw TSV benchmark files
├── model_performance/       # Model evaluation records
├── clip_features/           # CLIP embeddings for all questions
├── init/                    # Initial 10 seed questions (CLIP-based clustering)
├── results/                 # Output results
```


### 📦 Install
Git clone our repository, via the following command
```bash
git clone git@github.com:IMNearth/AutoJudger.git
cd AutoJudger
pip install -r requirements.txt
```

### 🔗 Prepare Data

1. Please manually download the original benchmark files (contains the raw TSV files with questions, images, options, answers) and place them in the `LMUData/` directory:
- [SEEDBench_IMG.tsv](https://opencompass.openxlab.space/utils/benchmarks/SEEDBench/SEEDBench_IMG.tsv) → `LMUData/SEEDBench_IMG.tsv`
- [AI2D_TEST.tsv](https://opencompass.openxlab.space/utils/VLMEval/AI2D_TEST.tsv) → `LMUData/AI2D_TEST.tsv`
- [MMMU_DEV_VAL.tsv](https://opencompass.openxlab.space/utils/VLMEval/MMMU_DEV_VAL.tsv) → `LMUData/MMMU_DEV_VAL.tsv`
- [MMT-Bench_VAL.tsv](https://opencompass.openxlab.space/utils/benchmarks/MMT-Bench/MMT-Bench_VAL.tsv) → `LMUData/MMT-Bench_VAL.tsv`


2. The `model_performance/` folder stores model records on these benchmarks. We thank the [VLMEvalKit](https://github.com/open-compass/VLMEvalKit) for providing these evaluation records.

3. The `data/` folder includes:
- Model list and benchmark metadata
- Train/test splits
- IRT-estimated difficulty scores for each question
  
  These can be generated by running `dataset.ipynb`.

4. The `clip_features/` and `init/` directories provide:
- CLIP embeddings for all questions
- 10 initial questions selected via CLIP-based clustering


### 🚀 Run
To launch an adaptive evaluation:
```bash
python main.py \
  --model_name Qwen2.5-VL-7B-Instruct \
  --benchmark SEEDBench_IMG \
  --feature text

# Parameters:
# --feature: Feature type [text|image|multimean|multiconcat]
# --benchmark: benchmark name
```

## 📊 Citation
If AutoJudger has been beneficial to your research and work, please cite our work using the following format:
```bibtex
@misc{ding2025autojudgeragentdrivenframeworkefficient,
      title={AutoJudger: An Agent-Driven Framework for Efficient Benchmarking of MLLMs}, 
      author={Xuanwen Ding and Chengjun Pan and Zejun Li and Jiwen Zhang and Siyuan Wang and Zhongyu Wei},
      year={2025},
      eprint={2505.21389},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2505.21389}, 
}
```

